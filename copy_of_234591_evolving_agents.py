# -*- coding: utf-8 -*-
"""Copy of 234591 -Evolving agents

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/112HXtKELj2jE8DGHrpj5s4i6T5v5Rbvd

<h1><center><b> Acquired Intelligence & Adaptive Behaviour </b></center></h1>
<h2><center><i>Evolving agents (Lab Sheet 03) </i></center></h2>

In this sheet we will take what we learned in the first two sheets to evolve a neural network which controls some agent. Your goal is to apply the GA your developed in the previous labs to this agent. The sheet includes an example of a single hillclimber which doesnt do very well. Once you have something working then experiment with different networks and different tasks.

We will be using the [`CartPole-v1`](https://gym.openai.com/envs/CartPole-v1/) task from `OpenAI` gym:


<center><img src="https://gym.openai.com/videos/2019-10-21--mqt8Qj1mwo/CartPole-v1/poster.jpg" width="300"></center>

> _A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center._


_If things stop working always try restarting the kernel, `opengl` does some strange things to Colab_

## Imports
(_We need to install some packages so we can render in Google colab_)
"""

!apt-get update
!apt install xvfb 
!apt-get install x11-utils > /dev/null 2>&1
!apt-get install -y xvfb python-opengl > /dev/null 2>&1
!pip install gym pyvirtualdisplay > /dev/null 2>&1

import os
import shutil
import random
import copy
import matplotlib.pyplot as plt
from IPython import display as ipythondisplay

import gym
from gym import logger as gymlogger
from gym.wrappers import Monitor
import numpy as np
import torch

"""## OpenAI gym

Let's take a basic look at a `gym` agent loop. Here, we will just use random actions. The following envs which are relatively simple:

- `Cartpole-v1`
- `Acrobot-v1`
- `MountainCar-v0`
- `Pendulum-v0`

Note each environment will accept different actions and observations (see the lecture). Some accept discrete (`int`) actions and observations, some accept continuous (`float`) actions and observations. Use `env.action_space` and `env.observation_space`.

Here we will use `CartPole-v1`:
"""

env_name = 'CartPole-v1'
env = gym.make(env_name) # create the gym environment
print(f"Action space {env.action_space} \nObservation space {env.observation_space}")

"""**Action:** This means that the environment accepts a single number as an action, which is a discrete number in $[0, 1]$. Here, 0 corresponds to 'push cart to the left' and 1 corresponds to 'push cart to the right'. But the whole point of the `gym` philosophy is that you don't need to know this. 

**Observation:** `Box` is a class from `gym`, it just means the observations will be wihtin some range (here -3.4 and 3.4), there will be 4 observations, and they will be continuous (`float32`). These 4 numbers corresponds to `

- position of cart
- velocity of cart
- angle of pole 
- rotation rate of pole 

But we had to Google that, good general-purpose agents don't need to know the semantics of the observations: they can learn how to map observations to actions to maximize reward without any prior knowledge.

Let's see how to run a `gym` environment. Two more concepts are needed here:

**Reset**: When you are learning agents, you need to run simulations hundreds (or billions if you are Google) times. Rather than create a new `env` instance each time, we can call `env.reset()` at the start of each simulation. 

**Done**: `gym` will tell you if the the simulations has finished. For instance, if you have died, or the time limit has been reached. You can also end the simulation yourself, but generally you rely on what `gym` says. Each time you call `env.step()` (coverered below) `gym` will tell you whether the simulations is finished or not

**Reward**: calling `env.step()` will also return how much reward you got for that time step. The amount of reward will vary for each environment, but all you need to know is that **more reward is better.** It will always a single floating point number. 

**Info**: `env.step()` also returns a dictionary called `info`. This sometimes contain additional information, but you can safely ignore it, 

**Step**: Brining this togetherm `env.step()` operates like this:

```python
obs, reward, done, info = env.step(action)
```

Here is an example ***(note I am just using random actions for now - there is no agent)***:
"""

env_name = 'CartPole-v1'
env = gym.make(env_name)

env.reset()
done = False
while not done:
    action = env.action_space.sample() # put agent here
    obs, reward, done, info = env.step(action)
    print(f"obs{np.round(obs, 2)} reward {reward} action {action} done {done} info {info}")

"""It's best practice to wrap a single episode (or simulation, run, whatever you want to call it) in a function. The function will take the agent which we will create below, and which will have a `get_action` function which will take the observations generated by the environment. This function then returns the total reward from that episode. It is a type of **fitness function**:"""

def run_episode(env, agent):
    tot_reward = 0.0
    obs = env.reset()
    done = False
    while not done:
        action = agent.get_action(obs)
        obs, reward, done, info = env.step(action)
        tot_reward = tot_reward + reward
    return tot_reward

"""## Visualising your agent

It is normally very simple to visualise your simulations. But Google Colab makes it very hard. Here, we have provided a wrapper function which will visualise your episode. You don't need to understand it, and it's slow, so don't use it during learning (use it to view your agent after learning). 

Passing an agent to the function is optional. If you don't pass an agent, it selects actions at random
"""

def visual_episode(env, agent=None):
    from pyvirtualdisplay import Display
    display = Display(visible=0, size=(400, 300))
    display.start()
    prev_screen = env.render(mode='rgb_array')
    plt.imshow(prev_screen)

    tot_reward = 0.0
    obs = env.reset()
    done = False
    curr_t = 0
    while not done:
        if agent is not None:
            action = agent.get_action(obs)
        else:
            action = env.action_space.sample()
        obs, reward, done, info = env.step(action)
        screen = env.render(mode='rgb_array')
        tot_reward = tot_reward + reward

        plt.imshow(screen)
        plt.title(curr_t)
        ipythondisplay.clear_output(wait=True)
        ipythondisplay.display(plt.gcf())
        curr_t = curr_t + 1

    ipythondisplay.clear_output(wait=True)
    env.close()
    return tot_reward

# This is the function call
visual_episode(env)

"""### Savind the simulation frames
Alternatively instead of displaying the frames on Google colab you can save them in a folder here and download them later.
You can look for the folder on the left hand sidebar. 
"""

def save_vis(env, agent=None, dirname='sim_frames'):
    if os.path.isdir(dirname):
      shutil.rmtree(dirname)
    os.makedirs(dirname, exist_ok=True)
    tot_reward = 0.0
    obs = env.reset()
    done = False
    curr_t = 0
    while not done:
        if agent is not None:
            action = agent.get_action(obs)
        else:
            action = env.action_space.sample()
        obs, reward, done, info = env.step(action)
        screen = env.render(mode='rgb_array')
        tot_reward = tot_reward + reward
        plt.imshow(screen)
        plt.title(curr_t)
        plt.savefig(os.path.join(dirname, 'fig{}'.format(curr_t)))
        curr_t += 1

# This is the function call
save_vis(env)

"""## Agent

We can now move on to the idea of agents. We will need to make a way to map genotypes to agents. Our agents will use neural networks to select actions, so a natural method is to have the genes encode the weights and biases of your neural network.

Our agent will take the number of inputs (the number of observations from the environment) and map it to the number of outputs (in this case, a single action).

We will use the `set_gene` function to map a list of floats (our genes) of length `num_genes` (see the `Agent` class) to a set of weights and biases.

The output of this neural network will be a continuous. We will therefore map the number to a discrete output (0 or 1) by thresholding.
The rest of this class should be familiar from the previous work sheets.


"""

class Agent:
    def __init__(self, num_input, num_output):
        self.num_input = num_input 
        self.num_output = num_output
        self.num_genes = num_input * num_output + num_output
        self.weights = None
        self.bias = None

    def set_genes(self, gene):
        weight_idxs = self.num_input * self.num_output
        bias_idxs = self.num_input * self.num_output + self.num_output
        w = gene[0 : weight_idxs].reshape(self.num_output, self.num_input)   
        b = gene[weight_idxs: bias_idxs].reshape(self.num_output,)
        self.weights = torch.from_numpy(w)
        self.bias = torch.from_numpy(b)

    def forward(self, x):
        x = torch.from_numpy(x).unsqueeze(0)
        return torch.mm(x, self.weights.T) + self.bias

    def get_action(self, x):
        if self.forward(x) > 0:
            return 1
        else:
            return 0

"""Creating our agent is simple:"""

num_obs = 4 # depends on your environment
num_actions = 1 # depends on your environment
agent = Agent(num_obs, num_actions)

"""## Learning your agent

We now need a way to change our genes in order to maximise reward. We will use the GA method, and you are encouraged to try different variants.

We can encode the populations of genes in the same way as previous weeks, The `Agent` class has a field called `num_genes`, this is how many genes you need. 

**Note that our gene are now continuous valued numbers, unlike last week**
"""

num_obs = 4 
num_actions = 1 
agent = Agent(num_obs, num_actions)

pop_size = 15
num_genes = agent.num_genes
gene_pop = np.random.normal(0, 0.1, (pop_size, num_genes))
# gene_pop = np.zeros((pop_size, num_genes))
print(f"{gene_pop.shape[0]} individuals in population, {gene_pop.shape[1]} genes per individual")

"""## Fitness function

We want to maximise the reward from running our simulations. For each genotype in our population, we map it to an agent, evaluate its reward, and return a list of rewards for each agent. 
"""

def fitness_function(env, agent, gene_pop):
    # Get fitness for each gene
    pop_size = gene_pop.shape[0]
    pop_fitness = np.zeros(pop_size)
    for i, gene in enumerate(gene_pop):
        agent.set_genes(gene)
        pop_fitness[i] = run_episode(env, agent)
    return pop_fitness

"""## Mutation 

We need some method for making mutations to genes. Below is an extremely simple example where we just add Gaussian noise, but we suggest you explore better methods
"""

def mutation_function(gene_pop, mean=0, std=0.1):
    gene_pop = np.copy(gene_pop)
    for i, gene in enumerate(gene_pop):
        gene_pop[i] = gene_pop[i] + np.random.normal(mean, std, size=gene.shape)
    # constraint
    gene_pop[gene_pop > 4] = 4
    gene_pop[gene_pop < -4] = -4

    return gene_pop

"""## Selection

We also need some way to determine whether our mutation was benefical. Below is a simple function which takes the mutated value if it was an improvement, otherwise keeps the original value
"""

def select_genes(prev_gene_pop, gene_pop, prev_fitness, fitness):
    new_gene_pop = np.zeros_like(gene_pop)
    for i, gene in enumerate(gene_pop): 
        if fitness[i] >= prev_fitness[i]: 
            new_gene_pop[i] = gene_pop[i]
        else:
            new_gene_pop[i] = prev_gene_pop[i]

    return new_gene_pop

"""# Best genotype

A function to get the best gene from a population. You may want to use that after the GA has run and you need to visualise the result.
"""

def get_best_gene(gene_pop, fitness):
    i = np.argmax(fitness)
    return gene_pop[i]

"""## Hill climbing"""

# Parameters
epochs, pop_size, mutation_std = 1000, 15, 0.01
num_obs, num_actions = 4, 1 

# Create our agent
agent = Agent(num_obs, num_actions)

# Create our gene population
gene_pop = np.random.normal(0, 0.1, (pop_size, agent.num_genes))

# Get init fitness of the population
fitness = fitness_function(env, agent, gene_pop)

prev_fitness = fitness
prev_gene_pop = gene_pop

# Main loop
for epoch in range(epochs):
    # Mutate our population
    gene_pop = mutation_function(prev_gene_pop, std=mutation_std)

    # Get fitness of the mutated population
    fitness = fitness_function(env, agent, gene_pop)

    # Apply selection 
    gene_pop = select_genes(prev_gene_pop, gene_pop, prev_fitness, fitness)

    # Evaluated the fitness of the selected population
    prev_fitness = fitness_function(env, agent, gene_pop)
    prev_gene_pop = gene_pop

    if epoch % 100 == 0:
        print(f"Mean fitness of population {fitness.mean()}")
        print(f"Max fitness of population {fitness.max()}")

"""## Visualise agent"""

# Visualise our agent or save frames to file
best_gene = get_best_gene(gene_pop, fitness)
agent.set_genes(best_gene)
visual_episode(env, agent)
save_vis(env, agent)

"""# Genetic Algorithm"""

class Agent:
    def __init__(self, num_input, num_output):
        self.num_input = num_input 
        self.num_output = num_output
        self.num_genes = num_input * num_output + num_output
        self.weights = None
        self.bias = None

    def set_genes(self, gene):
        weight_idxs = self.num_input * self.num_output
        bias_idxs = self.num_input * self.num_output + self.num_output
        w = gene[0 : weight_idxs].reshape(self.num_output, self.num_input)   
        b = gene[weight_idxs: bias_idxs].reshape(self.num_output,)
        self.weights = torch.from_numpy(w)
        self.bias = torch.from_numpy(b)

    def forward(self, x):
        x = torch.from_numpy(x).unsqueeze(0)
        return torch.mm(x, self.weights.T) + self.bias

    def get_action(self, x):
        if self.forward(x) > 0:
            return 1
        else:
            return 0


def visual_episode(env, agent=None):
    from pyvirtualdisplay import Display
    display = Display(visible=0, size=(400, 300))
    display.start()
    prev_screen = env.render(mode='rgb_array')
    plt.imshow(prev_screen)

    tot_reward = 0.0
    obs = env.reset()
    done = False
    curr_t = 0
    while not done:
        if agent is not None:
            action = agent.get_action(obs)
        else:
            action = env.action_space.sample()
        obs, reward, done, info = env.step(action)
        screen = env.render(mode='rgb_array')
        tot_reward = tot_reward + reward

        plt.imshow(screen)
        plt.title(curr_t)
        ipythondisplay.clear_output(wait=True)
        ipythondisplay.display(plt.gcf())
        curr_t = curr_t + 1

    ipythondisplay.clear_output(wait=True)
    env.close()
    return tot_reward

def save_vis(env, agent=None, dirname='sim_frames'):
    if os.path.isdir(dirname):
      shutil.rmtree(dirname)
    os.makedirs(dirname, exist_ok=True)
    tot_reward = 0.0
    obs = env.reset()
    done = False
    curr_t = 0
    while not done:
        if agent is not None:
            action = agent.get_action(obs)
        else:
            action = env.action_space.sample()
        obs, reward, done, info = env.step(action)
        screen = env.render(mode='rgb_array')
        tot_reward = tot_reward + reward
        plt.imshow(screen)
        plt.title(curr_t)
        plt.savefig(os.path.join(dirname, 'fig{}'.format(curr_t)))
        curr_t += 1


def run_episode(env, agent):
    tot_reward = 0.0
    obs = env.reset()
    done = False
    observations =[]
    while not done:
        action = agent.get_action(obs)
        obs, reward, done, info = env.step(action)
        tot_reward = tot_reward + reward
        observations.append(obs)
    return tot_reward,observations

def fitness_function(env, agent, genotype):
    # Get fitness of genotype
    agent.set_genes(genotype)
    
    fitness, ob = run_episode(env, agent)
    return fitness ,ob

def mutation_function(genotype, mean=0, std=0.1):
    genotype = np.copy(genotype)
    genotype = genotype + np.random.normal(mean, std, size=genotype.shape)

    # constraint
    for gene in genotype:
      if gene > 4:
        gene = 4
      elif gene < -4: 
        gene = -4

    return genotype

def select_genes(prev_genotype, genotype, prev_fitness, fitness):
    return prev_genotype if prev_fitness > fitness  else genotype

def get_best_gene(gene_pop, fitness):
    i = np.argmax(fitness)
    return gene_pop[i]


def pick_two(genotypes):
  i1 = np.random.randint(0, len(genotypes) - 1)
  i2 = np.random.randint(0, len(genotypes) - 1)
  while i1 == i2:
    i2 = np.random.randint(0, len(genotypes) - 1)
  return (np.copy(genotypes[i1]), np.copy(genotypes[i2])), (i1, i2)




def fitness_of_population(env, agent, gene_pop):
    # Get fitness for each gene
    pop_size = gene_pop.shape[0]
    pop_fitness = np.zeros(pop_size)
    obs = [0] * pop_size
    for i, gene in enumerate(gene_pop):
        agent.set_genes(gene)
        pop_fitness[i], obs[i] = run_episode(env, agent)
    return pop_fitness ,obs

import numpy as np
import random as rand


def mutation(genotype,mutation_rate):
  for i in range(len(genotype)):
    if mutation_rate > np.random.random():
      if genotype[i] == 1:
        genotype[i] = 0
      else:
        genotype[i] = 1

def local_neighbour(neighbourhood_size,index,populationsize):
  k = index + neighbourhood_size
  if k > populationsize:
    local_neighbour_idx = k - populationsize
  else:
    local_neighbour_idx = np.random.randint(index+1, index + neighbourhood_size) # pick random neighbour 
  #local_neighbour_gene = genotypes[local_neighbour_idx, :]
  return local_neighbour_idx


def Pcrossover(Wgenotype,Lgenotype,crossover_prob ):
  for i in range(len(Lgenotype)):
    if crossover_prob > np.random.random():
      Lgenotype[i] = Wgenotype[i]

  return Lgenotype
  


def getlargestarr(arrofarr):
  maxarr=[]
  for i in arrofarr:
    maxarr.append(max(i))
  return maxarr.index(max(maxarr))



def getWinner(genotypes, env, agent):
  assert len(genotypes) == 2

  if fitness_function(env, agent, genotypes[0])[0] > fitness_function(env, agent, genotypes[1])[0]:
    return genotypes[0] , genotypes[1]
  else:
    return genotypes[1], genotypes[0]


# lets say I want to sample a neighbour for indivudal 3
#print(local_neighbour_gene)
def runTournament(neighbourhood,crossover_prob,mutation_rate,genotypes,env,agent):

  genos, indexes = pick_two(genotypes)
  W,L = getWinner(genos,env,agent)
  L_prev,obs   = fitness_function(env, agent, L)
  #print("L- "+str(L_prev))
  Pcrossover(W,L,crossover_prob)
  mutated = mutation_function(L,std=mutation_rate)
  L_post,obs = fitness_function(env, agent, L)
  chosen_genes = select_genes(L, mutated, L_prev, L_post)
  #print("L- "+ str(L_post))
  genotypes[indexes[0]] = W
  genotypes[indexes[1]] = chosen_genes


  return genotypes

"""Algorithm

#Pole angle vs Time
Shows pole angle vs time 
shows each interval
"""

epochs, pop_size, mutation_std = 1000, 15, 0.01
num_obs, num_actions = 4, 1 
env_name = 'CartPole-v1'
env = gym.make(env_name) # create the gym environment
all_pop_fitnesses_max = []
size = plt.figure(figsize= (15,7))
size.suptitle("Pole Angle vs Time ")
# Create our agent
agent = Agent(num_obs, num_actions)


# Create our gene population

for j in range(1,4):
  all_pop_fitnesses_max = []
  all_obs = []
  allangles =[]
  all = []
  b = []

  genotypes = np.random.normal(0, 0.1, (pop_size, agent.num_genes))
  for i in range(0, epochs):
    runTournament(8,0.6,mutation_std,genotypes, env, agent)

    
    if i % 1   == 0:

      pop_fitness,all_obs= fitness_of_population(env, agent, genotypes)
      
      #print("Mean population fitness: {}".format(pop_fitness.mean()))
      #print("Max population fitness: {}".format(pop_fitness.max()))
      all_pop_fitnesses_max.append(pop_fitness.max())
  #plt.plot(all_pop_fitnesses_max, label = "T Run "+str(j))
  for b in range(len(all_obs[0])):
    for k in range(len(all_obs[0][b])):
      allangles.append(all_obs[0][b][2])
      #all.append(all_obs[0][j][0])
      #b.append(all_obs[0][j][3])
  #plt.plot(allangles, label = "a")
  plt.plot(allangles, label = j)
  #plt.plot(b, label = "c")

  plt.legend()

"""#Max Fitness vs Tournament runs"""

epochs, pop_size, mutation_std = 1000, 15, 0.01
num_obs, num_actions = 4, 1 
env_name = 'CartPole-v1'
env = gym.make(env_name) # create the gym environment
all_pop_fitnesses_max = []
size = plt.figure(figsize= (15,7))
size.suptitle("Max fitness vs Tournament runs ")
# Create our agent
agent = Agent(num_obs, num_actions)
all_obs = []

# Create our gene population

for j in range(1,4):
  all_pop_fitnesses_max = []
  genotypes = np.random.normal(0, 0.1, (pop_size, agent.num_genes))
  for i in range(0, epochs):
    runTournament(8,0.6,mutation_std,genotypes, env, agent)

    
    if i % 1   == 0:

      pop_fitness,all_obs = fitness_of_population(env, agent, genotypes)
      all_pop_fitnesses_max.append(pop_fitness.max())

    elif i % 100 == 0:
      print("Mean population fitness: {}".format(pop_fitness.mean()))
      print("Max population fitness: {}".format(pop_fitness.max()))
  plt.plot(all_pop_fitnesses_max, label = "T Run "+str(j))
  plt.legend()

"""#Mean Fitness vs Tournament runs"""

epochs, pop_size, mutation_std = 1000, 15, 0.01
num_obs, num_actions = 4, 1 
env_name = 'CartPole-v1'
env = gym.make(env_name) # create the gym environment
all_pop_fitnesses_max = []
size = plt.figure(figsize= (15,7))
size.suptitle("Mean fitness vs Tournament runs ")
# Create our agent
agent = Agent(num_obs, num_actions)
all_obs = []

# Create our gene population

for j in range(1,4):
  all_pop_fitnesses_max = []
  genotypes = np.random.normal(0, 0.1, (pop_size, agent.num_genes))
  for i in range(0, epochs):
    runTournament(8,0.6,mutation_std,genotypes, env, agent)

    
    if i % 1   == 0:

      pop_fitness,all_obs = fitness_of_population(env, agent, genotypes)
      all_pop_fitnesses_max.append(pop_fitness.mean())
    elif i % 100 == 0:
    print("Mean population fitness: {}".format(pop_fitness.mean()))
    print("Max population fitness: {}".format(pop_fitness.max()))
  plt.plot(all_pop_fitnesses_max, label = "T Run "+str(j))
  plt.legend()

epochs, pop_size, mutation_std = 1000, 15, 0.01
num_obs, num_actions = 4, 1 
env_name = 'CartPole-v1'
env = gym.make(env_name) # create the gym environment
all_pop_fitnesses_mean = []
size = plt.figure(figsize= (15,7))
size.suptitle("Mean fitness vs Tournament runs ")
# Create our agent
agent = Agent(num_obs, num_actions)
new_obs = []
# Create our gene population
genotypes = np.random.normal(0, 0.1, (pop_size, agent.num_genes))

for i in range(0, epochs):
  runTournament(8,0.6,mutation_std,genotypes, env, agent)

  
  if i % 1   == 0:

    pop_fitness,newobs = fitness_of_population(env, agent, genotypes)
    
    print("Mean population fitness: {}".format(pop_fitness.mean()))
    print("Max population fitness: {}".format(pop_fitness.max()))
    all_pop_fitnesses_mean.append(pop_fitness.mean())
plt.plot(all_pop_fitnesses_mean)

"""#The best Controller at Intervals of 0, 500 and 1000"""

epochs, pop_size, mutation_std = 1001, 15, 0.01
num_obs, num_actions = 4, 1 
env_name = 'CartPole-v1'
env = gym.make(env_name) # create the gym environment
all_pop_fitnesses_mean = []
size = plt.figure(figsize= (15,7))
x=["0","500","1000"]
size.suptitle("Mean fitness vs Tournament runs ")
# Create our agent
agent = Agent(num_obs, num_actions)
obs = obs
# Create our gene population
genotypes = np.random.normal(0, 0.1, (pop_size, agent.num_genes))

for i in range(0, epochs):
  runTournament(8,0.6,mutation_std,genotypes, env, agent)

  
  if i % 500   == 0:

    pop_fitness,obs = fitness_of_population(env, agent, genotypes)
    
    #print("Mean population fitness: {}".format(pop_fitness.mean()))
    #print("Max population fitness: {}".format(pop_fitness.max()))
    all_pop_fitnesses_mean.append(pop_fitness.max())
plt.bar(x,all_pop_fitnesses_mean)
#print(all_pop_fitnesses_mean)